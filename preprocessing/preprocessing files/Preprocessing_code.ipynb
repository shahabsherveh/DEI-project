{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8254a76c-2cb4-4232-a5c1-cc481e200d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5586cd7d-49a0-4070-a611-361e36b9cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditSentimentAnalysis\") \\\n",
    "    .master(\"spark://192.168.2.46:7077\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "    .config(\"spark.executor.cores\",2)\\\n",
    "    .config(\"spark.cores.max\", 4) \\\n",
    "    .config(\"spark.driver.port\",9999)\\\n",
    "    .config(\"spark.blockManager.port\",10005)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f45085e1-1ae6-4dc5-ac0c-4ad21e37fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"hdfs://192.168.2.46:9000/data/corpus-webis-tldr-17.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf7ba155-e0fa-4ebe-9326-738ef9b5c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only necessary columns\n",
    "df = df.select(\"author\", \"normalizedBody\", \"subreddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0f7cd34-4dd6-48f2-b21b-a93454b47468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bot names: ['ADHDbot', 'ALTcointip', 'AVR_Modbot', 'A_random_gif', 'AltCodeBot', 'Antiracism_Bot', 'ApiContraption', 'AssHatBot', 'AtheismModBot', 'AutoInsult']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|          author|      normalizedBody|  subreddit|\n",
      "+----------------+--------------------+-----------+\n",
      "|raysofdarkmatter|I think it should...|       math|\n",
      "|         Stork13|Art is about the ...|      funny|\n",
      "|   Cloud_dreamer|Ask me what I thi...|Borderlands|\n",
      "|   NightlyReaper|In Mechwarrior On...|   gamingpc|\n",
      "|  NuffZetPand0ra|You are talking a...|     Diablo|\n",
      "+----------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 1: Remove bots comments\n",
    "bot_df = pd.read_csv(\"/home/ubuntu/botlist.csv\")\n",
    "bot_list = bot_df[\"AAbot\"].dropna().unique().tolist()\n",
    "print(\"Sample bot names:\", bot_list[:10])  # Show the first 10 bot usernames\n",
    "# Remove bot-generated comments based on the bot list\n",
    "df_filtered = df.filter(~col(\"author\").isin(bot_list))\n",
    "df_filtered.show(5)  # Show filtered results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef9049c7-7e74-4705-a7fe-82430a2b8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n",
      "|          author|      normalizedBody|  subreddit|\n",
      "+----------------+--------------------+-----------+\n",
      "|raysofdarkmatter|I think it should...|       math|\n",
      "|         Stork13|Art is about the ...|      funny|\n",
      "|   Cloud_dreamer|Ask me what I thi...|Borderlands|\n",
      "|   NightlyReaper|In Mechwarrior On...|   gamingpc|\n",
      "|  NuffZetPand0ra|You are talking a...|     Diablo|\n",
      "+----------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove Null or Empty Text\n",
    "df = df.filter(col(\"normalizedBody\").isNotNull()).filter(col(\"normalizedBody\") != \"\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a82bc9f-3bdc-460a-aca4-4d84a8bde4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------------+\n",
      "|          author|  subreddit|          clean_text|\n",
      "+----------------+-----------+--------------------+\n",
      "|raysofdarkmatter|       math|i think it should...|\n",
      "|         Stork13|      funny|art is about the ...|\n",
      "|   Cloud_dreamer|Borderlands|ask me what i thi...|\n",
      "|   NightlyReaper|   gamingpc|in mechwarrior on...|\n",
      "|  NuffZetPand0ra|     Diablo|you are talking a...|\n",
      "+----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 2: Text Cleaning (Remove URLs, Special Characters, Markdown)\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \"\", text)  # Remove markdown links\n",
    "    text = re.sub(r\"[^A-Za-z0-9.,!? ]+\", \"\", text)  # Remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "# Register UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply cleaning function\n",
    "df = df.withColumn(\"clean_text\", clean_text_udf(col(\"normalizedBody\"))).drop(\"normalizedBody\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d856a60-ae78-4f65-8e1a-905362270f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------------+\n",
      "|          author|  subreddit|          clean_text|\n",
      "+----------------+-----------+--------------------+\n",
      "|raysofdarkmatter|       math|think fixed eithe...|\n",
      "|         Stork13|      funny|art hardest thing...|\n",
      "|   Cloud_dreamer|Borderlands|ask think wall st...|\n",
      "|   NightlyReaper|   gamingpc|mechwarrior onlin...|\n",
      "|  NuffZetPand0ra|     Diablo|talking charsi im...|\n",
      "+----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Step 3: Remove Stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Register UDF\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "\n",
    "# Apply stopword removal\n",
    "df = df.withColumn(\"clean_text\", remove_stopwords_udf(col(\"clean_text\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa006ea9-781e-4c20-9612-a5c45a76a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Register UDF\n",
    "lemmatize_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Apply lemmatization\n",
    "df = df.withColumn(\"clean_text\", lemmatize_udf(col(\"clean_text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8131e030-0e03-4c94-a8fc-48e15d9541fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenization\n",
    "def tokenize_text(text):\n",
    "    return \" \".join(word_tokenize(text))\n",
    "\n",
    "# Register UDF\n",
    "tokenize_udf = udf(tokenize_text, StringType())\n",
    "\n",
    "# Apply tokenization\n",
    "df = df.withColumn(\"clean_text\", tokenize_udf(col(\"clean_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fdd9121-de87-4242-ab1a-a8f9448a0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
