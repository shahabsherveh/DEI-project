Discuss in what way the role of data types and formats place on the
technological solutions. Are there specialized tools that achieve high-performance
and high usability for specific formats?

The data type and formats affects our data in two ways size and complexity. For instance, using type Float64 instead of Float32 would double the size of our data which could add to big data challenges namely data capture and storage, data transmission, data analysis and visualization [1].


- Challenges
  - Data Curation: Data curation is aimed at data discovery and retrieval, data quality assurance, value addition, reuse and preservation over
  time.
    - sub-fields: Authentication, archiving, management, preservation, retrieval and presentation
    - Data types and formats could affect the way our data is stored 
  - Data Analysis


# References

## Data-intensive applications, challenges, techniques and technologies: A survey on Big Data (ReviewBigData2013.pdf)
- Data curation is aimed at data discovery and retrieval, data quality assurance, value addition, reuse and preservation over
time. This field specifically involves a number of sub-fields including authentication, archiving, management, preservation,
retrieval, and representation. The existing database management tools are unable to process Big Data that grow so large and
complex. This situation will continue as the benefits of exploiting Big Data allowing researchers to analyse business trends,
prevent diseases, and combat crime. Though the size of Big Data keeps increasing exponentially, current capability to work
with is only in the relatively lower levels of petabytes, exabytes and zettabytes of data. 

- The classical approach of managing
structured data includes two parts, one is a schema to storage the data set, and another is a relational database for data re-
trieval. For managing large-scale datasets in a structured way, data warehouses and data marts are two popular approaches. A
data warehouse is a relational database system that is used to store and analyze data, also report the results to users. The
data mart is based on a data warehouse and facilitate the access and analysis of the data warehouse. A data warehouse is
mainly responsible to store data that is sourced from the operational systems. The preprocessing of the data is necessary
before it is stored, such as data cleaning, transformation and cataloguing. After these preprocessing, the data is available
for higher level online data mining functions. The data warehouse and marts are Standard Query Language (SQL) based dat-
abases systems.

- NoSQL employs a number of specific approaches. Firstly, 
data storage and management are separated into two independent parts. This is contrary to relational databases which try to
meet the concerns in the two sides simultaneously. This design gives NoSQL databases systems a lot of advantages. In the
storage part which is also called key-value storage, NoSQL focuses on the scalability of data storage with high-performance.
In the management part, NoSQL provides low-level access mechanism in which data management tasks can be implemented
in the application layer rather than having data management logic spread across in SQL or DB-specific stored procedure lan-
guages

- Most NoSQL databases have an important property. Namely, they are commonly schema-free. Indeed, the biggest advan-
tage of schema-free databases is that it enables applications to quickly modify the structure of data and does not need to
rewrite tables. 

- Additionally, it possesses greater flexibility when the structured data is heterogeneously stored. In the data
management layer, the data is enforced to be integrated and valid.

- Big Data techniques involve a number of disciplines, including statistics, data mining, machine learning, neural networks,
social network analysis, signal processing, pattern recognition, optimization methods and visualization approaches. 
- 

## A. Andrejev and T. Risch. Scientific sparql: Semantic web queries over Scientific Data

- Scientific data processing often involves complex
numerical data, such as multi-dimensional arrays and
functions over such numerical data. However, SPARQL does
not provide a general way of representing multi-dimensional
arrays. They have to be broken down into triples, which is
both unnatural and slow.
- Furthermore, scientific applications also require a rich set of functions operating over numerical
data, which are not part of SPARQL.

## Scientific Analysis by Queries in Extended SPARQL over a Scalable e-Science Data Store (AndrejAndrejev_IEEE2013.pdf)

- The use of relational databases in scientific computing
environments has been limited so far. One of the hindrances is
the need to design the database schema; an efficient use of a
database requires a certain level of expertise in data
engineering. Another key issue is the nature of the scientific
data itself: it frequently consists of complex data types such as
matrices and tensors, which are normally not handled by
relational databases. Instead, scientific data is often stored in
ad-hoc text or binary file formats or specialized databases, like
ROOT [10] or NetCDF [4].
-The RDF data model was initially proposed as a schema-
free alternative to relational and hierarchical databases, and is
so general that most other data models can be easily mapped
to RDF.
- SPARQL [7] is the W3C standard query language
for RDF.
- However, as we have shown in [8], to make the RDF model
useful and accepted in scientific applications one needs to
extend the RDF data model and the SPARQL query language
with capabilities to represent, search and process large
numeric arrays.
- Related Works for optimized for numeric arrays
  - SciSPARQL
  - SciDB
  - SciHadoop
  - ArrayStore
  - SciSPARQL extends SPARQL
with syntax and semantics for accessing numeric arrays of
arbitrary dimensionality, array slicing, projection and
transposition, and the ability to connect and invoke foreign
functions and define functional views.

## Challenges and Opportunities with Big Data (bigdatawhitepaper.pdf)

- Much data today is not natively in structured format; for example, tweets and blogs are 
weakly structured pieces of text, while images and video are structured for storage and display, but not 
for semantic content and search: transforming such content into a structured format for later analysis is 
a  major  challenge.
- The  value  of  data  explodes  when  it  can  be  linked  with  other  data,  thus  data  
integration is a major creator of value.
-Frequently,  the  information  collected  will  not  be  in  a  format  ready  for  analysis.    For  example,  
consider the collection of electronic health records in a hospital, comprising transcribed dictations from 
several  physicians,  structured  data  from  sensors  and  measurements  (possibly  with  some  associated  
uncertainty),  and  image  data  such  as  x-rays.  We  cannot  leave  the  data  in  this  form  and  still effectively  
analyze it.  Rather we require an information extraction process that pulls out the required information 
from  the  underlying  sources  and  expresses  it  in  a  structured  form  suitable  for  analysis.    Doing  this  
correctly  and  completely  is  a  continuing  technical  challenge.    Note  that  this  data  also  includes  images  
and will in the future include video; such extraction is often highly application dependent (e.g., what you 
want to pull out of an MRI is very different from what you would pull out of a picture of the stars, or a 
surveillance  photo).    In  addition,  due  to  the  ubiquity  of  surveillance  cameras  and  popularity  of  GPS-
enabled  mobile  phones,  cameras,  and  other  portable  devices,  rich  and  high  fidelity  location  and  
trajectory (i.e., movement in space) data can also be extracted.
- Given  the  heterogeneity of  the  flood  of  data,  it  is  not  enough  merely to  record  it  and  throw  it  
into a repository.   Consider, for example, data from a range of scientific experiments.  If we just have a 
bunch of data sets in a repository, it is unlikely anyone will ever be able to find, let alone reuse, any of 
this  data.    With  adequate  metadata,  there  is  some  hope,  but  even  so,  challenges  will  remain  due  to  
differences in experimental details and in data record structure.
- Data analysis is considerably more challenging than simply locating, identifying, understanding, 
and  citing  data.    For  effective  large-scale  analysis  all  of  this  has  to  happen  in  a  completely  automated  
manner.    This  requires  differences  in  data  structure  and  semantics  to  be  expressed  in  forms  that  are  
computer  understandable,  and  then  “robotically”  resolvable.    There  is  a  strong  body  of  work  in  data  
integration that can provide some of the answers.  However, considerable additional work is required to 
achieve automated error-free difference resolution.
- Even  for  simpler  analyses  that  depend  on  only  one  data  set,  there  remains  an  important  
question of suitable database design.  Usually, there will be many alternative ways in which to store the 
same information.  Certain designs will have advantages over others for certain purposes, and possibly 
drawbacks  for  other  purposes.    Witness,  for  instance,  the  tremendous  variety  in  the  structure  of  
bioinformatics databases with information regarding substantially similar entities, such as genes.  
Database  design  is  today  an  art,  and  is  carefully  executed  in  the  enterprise  context  by  highly-paid 
professionals.    We  must  enable  other  professionals,  such  as  domain  scientists,  to  create  effective  
database designs, either through devising tools to assist them in the design process or through forgoing 
the  design  process  completely  and  developing  techniques  so  that  databases  can  be  used  effectively  in  
the absence of intelligent database design.
- A problem with current Big Data analysis is the lack of coordination between database systems, 
which  host  the  data  and  provide  SQL  querying,  with  analytics  packages  that  perform  various  forms  of  
non-SQL  processing,  such  as  data  mining  and  statistical  analyses.

- Systems  with  a  rich  palette  of  visualizations  become  important  in  conveying  to  the  users  the  
results of the queries in a way that is best understood in the particular domain.  Whereas early business 
intelligence systems’ users were content with tabular presentations, today’s analysts need to pack and 
present  results  in  powerful  visualizations  that  assist  interpretation,  and  support  user  collaboration  as  
discussed in Sec. 3.5.
- However,  computer  systems  work  most  efficiently  if  they  can  store  multiple  items  that  are  all  
identical  in  size  and  structure.    Efficient  representation,  access,  and  analysis  of  semi-structured  data  
require further work. 

## The Researcher’s Guide to the Data Deluge: Querying a Scientific Database in Just a Few Seconds (VLDB_paper.pdf)

- Exploration speed and effectiveness can be
improved by the user using database statistics, sampling, synopsis,
and prior knowledge. Pre-canned queries and materialized views
aid new users in finding their way in the query space.
The situation in scientific databases however, is even more com-
plicated, because they often contain complex observation data, e.g.,
sky images or seismograms, and little a priori knowledge exists.
The prime challenge is to find models that capture the essence of
this data at both a macro- and a micro-scale. Paraphrased, the an-
swer is in the database, but the Nobel-price winning query is still
unknown.
- Solutions:
    - One-minute database kernels for real-time performance.
    - Multi-scale query processing for gradual exploration.
    - Result-set post processing for conveying meaningful data.
    - Query morphing to adjust for proximity results.
    - Query alternatives to cope with lack of providence.
    
### ONE-MINUTE DB KERNELS

- If the user accidentally produces a large result set, then a sam-
ple might be more informative and more feasible. Unfortunately,
such a sample depends on the data distribution, the correlations,
and data clustering in the database and the query result set. And
taking a sample can still cause significant performance degradation
that surface only at run time. An experienced user would resort
to querying a pre-computed database summary first. For scien-
tific databases though, even creating such summaries on the daily
stream of Terabytes becomes a challenge on its own.
- The first proposal is to address the problem at its root; we en-
vision one-minute database kernels that have rapid reactions on
user’s requests. Such a kernel differs from conventional kernels
by trying to identify and avoid performance degradation points on-
the-fly and to answer part of the query within strict time bounds, but
also without changing the query focus. Its execution plan should be
organized such that a (non-empty) answer can be produced within
T seconds.
- These ideas extends from high level design choices in database
operators and algorithms all the way to lower level (hardware con-
scious) implementation details.

### MULTI-SCALE QUERIES

- As the user becomes more and
more confident on the direction of his exploration he is willing to
analyze more data and spend more of his budget.
- The key challenge is to find the cost-metrics and database statis-
tics that allow the system to break queries into multiple steps. Opti-
mization techniques may focus on preparatory subqueries and pass-
ing intermediates between the stages.

### RESULT-SET POST PROCESSING

- To provide even more informative answers may call for step-
ping even further away from the target expression in a SQL query.
The guiding example query Q with millions of rows could perhaps
be compressed into a more meaningful way.

### QUERY MORPHING

- The directions seen so far target performance improvements re-
garding the queries posed. In a science exploratory setting though
even choosing the proper queries becomes a challenge. Even with a
time-aware kernel and data exploration with multi-stage querying,
badly chosen queries will still significantly hinder data exploration.
Perhaps they are formulated wrongly, or they get stuck in a part of
the database where no satisfying results exist.
- For this recurring situation, we introduce the notion of query
morphing as an integral part of query evaluation. It works as fol-
lows, the user gives a starting query Q and most of the effort T is
spent on finding the “best” answer for Q. But a small portion is set
aside for the following exploratory step. The query is syntactically
adjusted to create variations Qi , i.e., with a small edit distance from
Q. The process of query morphing is visualized on the left part
of the above figure.

### QUERIES AS ANSWERS

- No matter how well we deploy the techniques sketched so far,
they still do not address the user’s lack of knowledge about the
data. With huge data sets arriving on a daily basis, scientists do not
always have a clear view on the data characteristics or even what
they are looking for.
- It seems prudent to propose that the database system itself can
provide starting points and guidance for data exploration; instead
of returning results for a random or badly formulated query, the
system can return query suggestions for more effective exploration.
